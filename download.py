import os
import requests
import re
import shutil
import concurrent.futures
from collections import defaultdict
from tqdm import tqdm

# --- ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­å®š (èª¿æ•´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ---
# åŒæ™‚ã«å®Ÿè¡Œã™ã‚‹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã®æœ€å¤§æ•°ã€‚PCã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç’°å¢ƒã«å¿œã˜ã¦èª¿æ•´ (8-16ãŒä¸€èˆ¬çš„)
MAX_WORKERS = 12
# 1ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ„ã®ã‚µã‚¤ã‚º(MB)ã€‚å®‰å®šã—ãªã„å ´åˆã¯å°ã•ãã™ã‚‹ (5-20MBãŒä¸€èˆ¬çš„)
PART_SIZE_MB = 16


def query_database(session, database_id, headers):
    """Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€æŒ‡å®šã‚¿ã‚°ã‚’ä¸¡æ–¹æŒã¤ãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚"""
    print("ğŸ” Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ 'Python' ãŠã‚ˆã³ 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡' ã‚¿ã‚°ã‚’æŒã¤ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...")
    url = f"https://api.notion.com/v1/databases/{database_id}/query"
    query = {
        "filter": {"and": [{"property": "ã‚¿ã‚°", "multi_select": {"contains": "Python"}}, {"property": "ã‚¿ã‚°", "multi_select": {"contains": "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡"}}]},
        "sorts": [{"property": "ä½œæˆæ—¥æ™‚", "direction": "ascending"}]
    }
    all_results, has_more, next_cursor = [], True, None
    while has_more:
        if next_cursor: query["start_cursor"] = next_cursor
        res = session.post(url, headers=headers, json=query) # session ã‚’ä½¿ç”¨
        res.raise_for_status()
        data = res.json()
        all_results.extend(data["results"])
        has_more = data["has_more"]
        next_cursor = data["next_cursor"]
    if not all_results: print("âœ… æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else: print(f"âœ… {len(all_results)} ä»¶ã®ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    return all_results

def get_block_children(session, page_id, headers):
    """æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸IDã«å±ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚"""
    url = f"https://api.notion.com/v1/blocks/{page_id}/children"
    res = session.get(url, headers=headers) # session ã‚’ä½¿ç”¨
    return res.json()["results"] if res.ok else None

def update_page_tags(session, page_id, current_tags, downloaded_tag_name, headers):
    """ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚°ã‹ã‚‰ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡ã€ã‚’å‰Šé™¤ã—ã€ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã€ã‚’è¿½åŠ ã—ã¾ã™ã€‚"""
    new_tags = [tag for tag in current_tags if tag.get("name") != "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡"]
    if not any(tag.get("name") == downloaded_tag_name for tag in new_tags):
        new_tags.append({"name": downloaded_tag_name})

    original_tag_set = {(t.get('id'), t.get('name')) for t in current_tags}
    new_tag_set = {(t.get('id'), t.get('name')) for t in new_tags}
    if original_tag_set == new_tag_set: return

    url = f"https://api.notion.com/v1/pages/{page_id}"
    payload = {"properties": {"ã‚¿ã‚°": {"multi_select": new_tags}}}
    try:
        res = session.patch(url, headers=headers, json=payload) # session ã‚’ä½¿ç”¨
        res.raise_for_status()
        print(f"  - ãƒšãƒ¼ã‚¸(ID: {page_id})ã®ã‚¿ã‚°ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
    except requests.exceptions.RequestException as e:
        print(f"  âŒ ãƒšãƒ¼ã‚¸(ID: {page_id})ã®ã‚¿ã‚°æ›´æ–°ã«å¤±æ•—: {e}")

def download_part(session, url, part_path, byte_range, pbar):
    """å˜ä¸€ã®ãƒ‘ãƒ¼ãƒ„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹é–¢æ•°ã€‚ThreadPoolExecutorã‹ã‚‰å‘¼ã°ã‚Œã‚‹ã€‚"""
    try:
        headers = {'Range': f"bytes={byte_range[0]}-{byte_range[1]}"}
        with session.get(url, headers=headers, stream=True, timeout=60) as res:
            res.raise_for_status()
            with open(part_path, 'wb') as f:
                for chunk in res.iter_content(chunk_size=8192):
                    f.write(chunk)
                    pbar.update(len(chunk))
        return (part_path, True)
    except Exception as e:
        pbar.write(f"  - ãƒ‘ãƒ¼ãƒ„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {os.path.basename(part_path)} - {e}")
        return (part_path, False)

def get_task_size(session, task):
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—ã—ã€ã‚¿ã‚¹ã‚¯è¾æ›¸ã«è¿½åŠ ã—ã¦è¿”ã™"""
    try:
        with session.get(task['url'], stream=True, allow_redirects=True, timeout=15) as res:
            res.raise_for_status()
            task['size'] = int(res.headers.get('content-length', 0))
    except requests.exceptions.RequestException:
        task['size'] = 0
    return task

def main(config):
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # --- GUIã‹ã‚‰æ¸¡ã•ã‚ŒãŸè¨­å®šå€¤ã‚’èª­ã¿è¾¼ã‚€ ---
    NOTION_TOKEN = config.get("NOTION_TOKEN")
    DATABASE_ID = config.get("DATABASE_ID")
    DOWNLOAD_FOLDER_PATH = config.get("DOWNLOAD_FOLDER_PATH")
    NOTION_VERSION = config.get("NOTION_VERSION", "2022-06-28")
    DESCRIPTION_PROP_NAME = config.get("NOTION_DESCRIPTION_PROPERTY_NAME", "èª¬æ˜")
    NOTION_DOWNLOADED_TAG_NAME = config.get("NOTION_DOWNLOADED_TAG_NAME", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿")

    if not all([NOTION_TOKEN, DATABASE_ID, DOWNLOAD_FOLDER_PATH, DESCRIPTION_PROP_NAME]):
        print("ã‚¨ãƒ©ãƒ¼: å¿…é ˆé …ç›®(Token, DB ID, èª¬æ˜ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£å, ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å…ˆãƒ•ã‚©ãƒ«ãƒ€)ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    HEADERS = {"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION, "Content-Type": "application/json"}
    os.makedirs(DOWNLOAD_FOLDER_PATH, exist_ok=True)
    temp_chunk_dir = os.path.join(DOWNLOAD_FOLDER_PATH, "temp_chunks")

    # requests.Session() ã‚’ä½¿ç”¨ã—ã¦æ¥ç¶šã‚’å†åˆ©ç”¨
    with requests.Session() as session:
        try:
            os.makedirs(temp_chunk_dir, exist_ok=True)
            pages = query_database(session, DATABASE_ID, HEADERS)
            if not pages: return

            tasks_to_size_check, chunk_files_info = [], defaultdict(list)
            print("ğŸ“‹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")
            for page in pages:
                page_id, props = page["id"], page["properties"]
                desc_prop = props.get(DESCRIPTION_PROP_NAME)
                if not (desc_prop and desc_prop.get("rich_text")): continue
                description = desc_prop["rich_text"][0]["plain_text"]
                match = re.search(r"^(.*?) \(chunk (\d+) of (\d+)\)$", description)
                if not match: continue
                
                original_filename, chunk_num, total_chunks = match.groups()
                chunk_num, total_chunks = int(chunk_num), int(total_chunks)
                blocks = get_block_children(session, page_id, HEADERS)
                if not blocks: continue
                
                file_url = next((b["video"]["file"]["url"] for b in blocks if b["type"] == "video" and b.get("video", {}).get("type") == "file"), None)
                if not file_url: continue

                tags = props.get("ã‚¿ã‚°", {}).get("multi_select", [])
                task_info = {'url': file_url, 'page_id': page_id, 'tags': tags}
                if total_chunks == 1:
                    task_info['path'] = os.path.join(DOWNLOAD_FOLDER_PATH, original_filename)
                else:
                    chunk_filename = f"{original_filename}.part{chunk_num:03d}"
                    task_info['path'] = os.path.join(temp_chunk_dir, chunk_filename)
                    chunk_files_info[original_filename].append(task_info)
                tasks_to_size_check.append(task_info)

            if not tasks_to_size_check:
                print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            print(f"\nğŸ“Š {len(tasks_to_size_check)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆè¨ˆã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™...")
            download_tasks = []
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = [executor.submit(get_task_size, session, task) for task in tasks_to_size_check]
                for future in tqdm(concurrent.futures.as_completed(futures), total=len(tasks_to_size_check), desc="ã‚µã‚¤ã‚ºè¨ˆç®—é€²æ—"):
                    download_tasks.append(future.result())

            total_download_size = sum(task.get('size', 0) for task in download_tasks)
            if total_download_size == 0:
                print("\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                return

            print(f"\nâš¡ åˆè¨ˆ {total_download_size / 1024**3:.2f} GB ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
            
            # --- ä¸¦åˆ—å‡¦ç†ã®æ–°ã—ã„æ§‹é€  ---
            all_parts_to_download = []
            files_to_reconstruct = defaultdict(list)

            part_size = PART_SIZE_MB * 1024 * 1024
            for task in download_tasks:
                if task.get('size', 0) == 0: continue
                
                final_path = task['path']
                if os.path.exists(final_path):
                    print(f"  - ã‚¹ã‚­ãƒƒãƒ—: '{os.path.basename(final_path)}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
                    total_download_size -= task['size'] # å…¨ä½“é€²æ—ã‹ã‚‰é™¤å¤–
                    continue

                temp_dir = final_path + ".parts"
                os.makedirs(temp_dir, exist_ok=True)
                
                num_parts = (task['size'] + part_size - 1) // part_size
                files_to_reconstruct[final_path] = {
                    'parts': [os.path.join(temp_dir, f"part_{i:04d}") for i in range(num_parts)],
                    'task_info': task
                }

                for i in range(num_parts):
                    start_byte = i * part_size
                    end_byte = min((i + 1) * part_size - 1, task['size'] - 1)
                    part_path = os.path.join(temp_dir, f"part_{i:04d}")
                    all_parts_to_download.append({
                        'url': task['url'],
                        'path': part_path,
                        'range': (start_byte, end_byte)
                    })

            if not all_parts_to_download:
                print("\nå…¨ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™ã€‚")
                return

            # å˜ä¸€ã®ThreadPoolExecutorã§å…¨ãƒ‘ãƒ¼ãƒ„ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            with tqdm(total=total_download_size, unit='iB', unit_scale=True, desc="å…¨ä½“é€²æ—") as pbar:
                with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                    future_to_part = {executor.submit(download_part, session, part['url'], part['path'], part['range'], pbar): part for part in all_parts_to_download}
                    
                    for future in concurrent.futures.as_completed(future_to_part):
                        part_path, success = future.result()
                        # å¤±æ•—ã—ãŸå ´åˆã¯é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã®å†æ§‹ç¯‰ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
                        if not success:
                            for final_path, data in list(files_to_reconstruct.items()):
                                if part_path in data['parts']:
                                    pbar.write(f"\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãŸã‚ã€'{os.path.basename(final_path)}'ã®å‡¦ç†ã‚’ä¸­æ–­ã€‚")
                                    del files_to_reconstruct[final_path]

            print("\n--- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒ•ã‚¡ã‚¤ãƒ«ã®çµåˆã¨ã‚¿ã‚°æ›´æ–° ---")
            for final_path, data in files_to_reconstruct.items():
                print(f"  - çµåˆä¸­: '{os.path.basename(final_path)}'")
                try:
                    with open(final_path, 'wb') as dest_file:
                        for part_path in data['parts']:
                            with open(part_path, 'rb') as src_file:
                                shutil.copyfileobj(src_file, dest_file)
                    # çµåˆæˆåŠŸå¾Œã€é–¢é€£ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚°ã‚’æ›´æ–°
                    task_info = data['task_info']
                    update_page_tags(session, task_info['page_id'], task_info['tags'], NOTION_DOWNLOADED_TAG_NAME, HEADERS)
                finally:
                    shutil.rmtree(os.path.dirname(data['parts'][0])) # .parts ãƒ•ã‚©ãƒ«ãƒ€ã‚’å‰Šé™¤

        finally:
            if os.path.exists(temp_chunk_dir) and not os.listdir(temp_chunk_dir):
                print("\nğŸ§¹ ç©ºã®ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ 'temp_chunks' ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
                os.rmdir(temp_chunk_dir)
            print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")