import os
import requests
import re
import shutil
import concurrent.futures
from collections import defaultdict
from dotenv import load_dotenv
from tqdm import tqdm

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# --- ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­å®š ---
NOTION_TOKEN = os.getenv("NOTION_TOKEN")
DATABASE_ID = os.getenv("DATABASE_ID")
NOTION_VERSION = os.getenv("NOTION_VERSION", "2022-06-28")
DESCRIPTION_PROP_NAME = os.getenv("NOTION_DESCRIPTION_PROPERTY_NAME")
DOWNLOAD_FOLDER_PATH = os.getenv("DOWNLOAD_FOLDER_PATH")
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†æ™‚ã«ä»˜ä¸ã™ã‚‹ã‚¿ã‚°ã®åå‰
NOTION_DOWNLOADED_TAG_NAME = os.getenv("NOTION_DOWNLOADED_TAG_NAME", "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿")
# åŒæ™‚ã«å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®æœ€å¤§æ•°
MAX_WORKERS = 8
# 1ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ†å‰²ã™ã‚‹éš›ã®ã€1ãƒ‘ãƒ¼ãƒ„ã‚ãŸã‚Šã®ã‚µã‚¤ã‚º(MB)
PART_SIZE_MB = 10

# --- APIãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ãƒ˜ãƒƒãƒ€ãƒ¼ ---
HEADERS = {"Authorization": f"Bearer {NOTION_TOKEN}", "Notion-Version": NOTION_VERSION, "Content-Type": "application/json"}

def query_database():
    """Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã€æŒ‡å®šã‚¿ã‚°ã‚’ä¸¡æ–¹æŒã¤ãƒšãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚"""
    print("ğŸ” Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ 'Python' ãŠã‚ˆã³ 'ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡' ã‚¿ã‚°ã‚’æŒã¤ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ã„ã¾ã™...")
    url = f"https://api.notion.com/v1/databases/{DATABASE_ID}/query"
    query = {
        "filter": {"and": [{"property": "ã‚¿ã‚°", "multi_select": {"contains": "Python"}}, {"property": "ã‚¿ã‚°", "multi_select": {"contains": "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡"}}]},
        "sorts": [{"property": "ä½œæˆæ—¥æ™‚", "direction": "ascending"}]
    }
    all_results = []
    has_more = True
    next_cursor = None
    while has_more:
        if next_cursor: query["start_cursor"] = next_cursor
        res = requests.post(url, headers=HEADERS, json=query)
        res.raise_for_status()
        data = res.json()
        all_results.extend(data["results"])
        has_more = data["has_more"]
        next_cursor = data["next_cursor"]
    if not all_results: print("âœ… æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒšãƒ¼ã‚¸ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else: print(f"âœ… {len(all_results)} ä»¶ã®ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
    return all_results

def get_block_children(page_id):
    """æŒ‡å®šã•ã‚ŒãŸãƒšãƒ¼ã‚¸IDã«å±ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚"""
    url = f"https://api.notion.com/v1/blocks/{page_id}/children"
    res = requests.get(url, headers=HEADERS);
    return res.json()["results"] if res.ok else None

def update_page_tags(page_id, current_tags, downloaded_tag_name):
    """ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚°ã‹ã‚‰ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡ã€ã‚’å‰Šé™¤ã—ã€ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã€ã‚’è¿½åŠ ã—ã¾ã™ã€‚"""
    # ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡ã€ã‚’é™¤å¤–ã—ã€æ—¢ã«ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã€ãŒãªã„ã‹ç¢ºèª
    new_tags = [tag for tag in current_tags if tag.get("name") != "ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾…ã¡"]
    has_downloaded_tag = any(tag.get("name") == downloaded_tag_name for tag in new_tags)

    # ã€Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã€ã‚¿ã‚°ãŒãªã‘ã‚Œã°è¿½åŠ 
    if not has_downloaded_tag:
        new_tags.append({"name": downloaded_tag_name})

    # ã‚¿ã‚°æ§‹æˆãŒå¤‰æ›´ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿APIã‚’å‘¼ã³å‡ºã™
    # (å…ƒã®ã‚¿ã‚°ãƒªã‚¹ãƒˆã¨IDã¨åå‰ã®ã‚»ãƒƒãƒˆã‚’æ¯”è¼ƒ)
    original_tag_set = { (t.get('id'), t.get('name')) for t in current_tags }
    new_tag_set = { (t.get('id'), t.get('name')) for t in new_tags }
    if original_tag_set == new_tag_set:
        return

    url = f"https://api.notion.com/v1/pages/{page_id}"
    payload = { "properties": { "ã‚¿ã‚°": { "multi_select": new_tags } } }
    try:
        res = requests.patch(url, headers=HEADERS, json=payload)
        res.raise_for_status()
        print(f"  - ãƒšãƒ¼ã‚¸(ID: {page_id})ã®ã‚¿ã‚°ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")
    except requests.exceptions.RequestException as e:
        print(f"  âŒ ãƒšãƒ¼ã‚¸(ID: {page_id})ã®ã‚¿ã‚°æ›´æ–°ã«å¤±æ•—: {e}")

def download_file_multipart(url, final_path, total_size, pbar):
    """å˜ä¸€ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°ãƒ‘ãƒ¼ãƒ„ã«åˆ†å‰²ã—ã€ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"""
    if os.path.exists(final_path):
        pbar.write(f"  - ã‚¹ã‚­ãƒƒãƒ—: '{os.path.basename(final_path)}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
        pbar.update(total_size)
        return True

    part_size = PART_SIZE_MB * 1024 * 1024
    num_parts = (total_size + part_size - 1) // part_size
    temp_dir = final_path + ".parts"
    os.makedirs(temp_dir, exist_ok=True)
    
    tasks = [{'range': (i * part_size, min((i + 1) * part_size - 1, total_size - 1)),
              'path': os.path.join(temp_dir, f"part_{i:04d}"), 'url': url}
             for i in range(num_parts)]

    download_success = True
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        def _download_part(part_task):
            try:
                headers = {'Range': f"bytes={part_task['range'][0]}-{part_task['range'][1]}"}
                res = requests.get(part_task['url'], headers=headers, stream=True, timeout=60)
                res.raise_for_status()
                with open(part_task['path'], 'wb') as f:
                    for chunk in res.iter_content(chunk_size=8192):
                        f.write(chunk)
                        pbar.update(len(chunk))
                return True
            except Exception:
                return False

        futures = [executor.submit(_download_part, task) for task in tasks]
        for future in concurrent.futures.as_completed(futures):
            if not future.result(): download_success = False

    if not download_success:
        pbar.write(f"\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ã®ãŸã‚ã€'{os.path.basename(final_path)}'ã®å‡¦ç†ã‚’ä¸­æ–­ã€‚")
        shutil.rmtree(temp_dir)
        return False

    try:
        with open(final_path, 'wb') as dest_file:
            for i in range(num_parts):
                part_path = os.path.join(temp_dir, f"part_{i:04d}")
                with open(part_path, 'rb') as src_file: shutil.copyfileobj(src_file, dest_file)
    except IOError as e:
        pbar.write(f"  âŒ ãƒ•ã‚¡ã‚¤ãƒ«çµåˆå¤±æ•—: {e}")
        return False
    finally:
        shutil.rmtree(temp_dir)
    return True

def reconstruct_file_from_chunks(chunk_paths, final_path):
    """Notionã®ä»•æ§˜ã§åˆ†å‰²ã•ã‚ŒãŸãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’ã€ä¸€ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«çµåˆã—ã¦å¾©å…ƒã—ã¾ã™ã€‚"""
    print(f"\n  - Notionãƒãƒ£ãƒ³ã‚¯ã‚’çµåˆä¸­... -> '{os.path.basename(final_path)}'")
    chunk_paths.sort()
    with open(final_path, 'wb') as final_file:
        for chunk_path in chunk_paths:
            with open(chunk_path, 'rb') as chunk_file: shutil.copyfileobj(chunk_file, final_file)
    for chunk_path in chunk_paths: os.remove(chunk_path)
    print(f"  âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã®å¾©å…ƒãŒå®Œäº†ã—ã¾ã—ãŸ: {final_path}")

def get_task_size(task):
    """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’å–å¾—ã—ã€ã‚¿ã‚¹ã‚¯è¾æ›¸ã«è¿½åŠ ã—ã¦è¿”ã™"""
    try:
        with requests.get(task['url'], stream=True, allow_redirects=True, timeout=15) as res:
            res.raise_for_status()
            task['size'] = int(res.headers.get('content-length', 0))
    except requests.exceptions.RequestException:
        task['size'] = 0
    return task

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    os.makedirs(DOWNLOAD_FOLDER_PATH, exist_ok=True)
    temp_chunk_dir = os.path.join(DOWNLOAD_FOLDER_PATH, "temp_chunks")

    try:
        os.makedirs(temp_chunk_dir, exist_ok=True)
        pages = query_database()
        if not pages: return

        # 1. ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
        tasks_to_size_check = []
        chunk_files_info = defaultdict(list)
        
        print("ğŸ“‹ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ã„ã¾ã™...")
        for page in pages:
            page_id, properties = page["id"], page["properties"]
            desc_prop = properties.get(DESCRIPTION_PROP_NAME)
            if not desc_prop or not desc_prop["rich_text"]: continue
            description = desc_prop["rich_text"][0]["plain_text"]
            match = re.search(r"^(.*?) \(chunk (\d+) of (\d+)\)$", description)
            if not match: continue
            original_filename, chunk_num, total_chunks = match.groups()
            chunk_num, total_chunks = int(chunk_num), int(total_chunks)
            blocks = get_block_children(page_id)
            if not blocks: continue
            file_url = next((b["video"]["file"]["url"] for b in blocks if b["type"] == "video" and b["video"]["type"] == "file"), None)
            if not file_url: continue

            tags = properties.get("ã‚¿ã‚°", {}).get("multi_select", [])
            task_info = {'url': file_url, 'page_id': page_id, 'tags': tags}

            if total_chunks == 1:
                task_info['path'] = os.path.join(DOWNLOAD_FOLDER_PATH, original_filename)
            else:
                chunk_filename = f"{original_filename}.part{chunk_num:03d}"
                task_info['path'] = os.path.join(temp_chunk_dir, chunk_filename)
                chunk_files_info[original_filename].append(task_info)
            
            tasks_to_size_check.append(task_info)

        if not tasks_to_size_check:
            print("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # 2. å…¨ã‚¿ã‚¹ã‚¯ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ä¸¦åˆ—ã§å–å¾—
        print(f"\nğŸ“Š {len(tasks_to_size_check)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆè¨ˆã‚µã‚¤ã‚ºã‚’è¨ˆç®—ã—ã¦ã„ã¾ã™...")
        download_tasks = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(get_task_size, task) for task in tasks_to_size_check]
            for future in tqdm(concurrent.futures.as_completed(futures), total=len(tasks_to_size_check), desc="ã‚µã‚¤ã‚ºè¨ˆç®—é€²æ—"):
                download_tasks.append(future.result())

        total_download_size = sum(task.get('size', 0) for task in download_tasks)
        if total_download_size == 0:
            print("\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
            return

        # 3. å…¨ã‚¿ã‚¹ã‚¯ã®ä¸¦åˆ—ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
        print(f"\nâš¡ åˆè¨ˆ {total_download_size / 1024**3:.2f} GB ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™...")
        successful_tasks = []
        with tqdm(total=total_download_size, unit='iB', unit_scale=True, desc="å…¨ä½“é€²æ—") as pbar:
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_task = {executor.submit(download_file_multipart, task['url'], task['path'], task['size'], pbar): task
                                  for task in download_tasks if task.get('size', 0) > 0}
                
                for future in concurrent.futures.as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        if future.result():
                            successful_tasks.append(task)
                    except Exception as e:
                        pbar.write(f"ã‚¿ã‚¹ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {task['path']} - {e}")

        # 4. å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã®ã‚¿ã‚°ã‚’æ›´æ–°
        print("\n--- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚°ã‚’æ›´æ–°ã—ã¾ã™ ---")
        for task in successful_tasks:
            is_chunk = any(task in chunk_list for chunk_list in chunk_files_info.values())
            if not is_chunk:
                update_page_tags(task['page_id'], task['tags'], NOTION_DOWNLOADED_TAG_NAME)

        # 5. Notionãƒãƒ£ãƒ³ã‚¯ã®çµåˆã¨ã‚¿ã‚°æ›´æ–°
        if chunk_files_info:
            print("\n--- Notionãƒãƒ£ãƒ³ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®çµåˆå‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™ ---")
            for filename, chunk_task_list in chunk_files_info.items():
                chunk_paths = [task['path'] for task in chunk_task_list]
                if all(os.path.exists(p) for p in chunk_paths):
                    print(f"ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã®ãƒãƒ£ãƒ³ã‚¯ãŒå…¨ã¦æƒã„ã¾ã—ãŸã€‚")
                    final_path = os.path.join(DOWNLOAD_FOLDER_PATH, filename)
                    reconstruct_file_from_chunks(chunk_paths, final_path)
                    
                    print(f"  - '{filename}' ã«é–¢é€£ã™ã‚‹ãƒšãƒ¼ã‚¸ã®ã‚¿ã‚°ã‚’æ›´æ–°ã—ã¾ã™...")
                    for task_info in chunk_task_list:
                        update_page_tags(task_info['page_id'], task_info['tags'], NOTION_DOWNLOADED_TAG_NAME)
                else:
                    print(f"âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ« '{filename}' ã¯ãƒãƒ£ãƒ³ã‚¯ãŒä¸è¶³ã—ã¦ã„ã‚‹ãŸã‚çµåˆãƒ»ã‚¿ã‚°æ›´æ–°ã§ãã¾ã›ã‚“ã€‚")
    
    finally:
        # 6. æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç†
        if os.path.exists(temp_chunk_dir) and not os.listdir(temp_chunk_dir):
            print("\nğŸ§¹ ç©ºã®ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ 'temp_chunks' ã‚’å‰Šé™¤ã—ã¾ã™ã€‚")
            os.rmdir(temp_chunk_dir)
        print("\nğŸ‰ å…¨ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()
